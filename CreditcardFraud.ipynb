{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8365d747",
   "metadata": {},
   "source": [
    "# Predicting Credit Card Fraud Detetction with a Multi-layer Perceptron\n",
    "\n",
    "Dataset: Kaggle Credit Card Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e9710",
   "metadata": {},
   "source": [
    "**What is MLP and how is it similar to taught course content such as logistic regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e77b0",
   "metadata": {},
   "source": [
    "MLP stands for multi-layer perceptron. It is a feedforward neural network made of an input layer and one or more hidden hidden layers as well as an output layer. Each neuron takes a weighted sum of its inputs, adds a bias and passes it through a non-linear activation function. The weighting between each neuron is trained using backpropogation.\n",
    "\n",
    "Both MLP and logistic regression both compute a linear combination of inputs and pass it through a function that takes the output to a probability for the positive class. A single layer MLP with a sigmoid function is logistic regression. MLP adds many of these which creates a stack of logistic units that are better at estimating very non-linear relationships. Both Logistic regression and a MLP can be trained using gradient descent on a loss function. With MLP the gradient has to be pushed back through the multiple layers to alter the weights. Scikit-learn solvers such as adam, which will be used here, is a type of gradient descent optimiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193fe156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "     ---------------------------------------- 0.0/52.8 kB ? eta -:--:--\n",
      "     ---------------------- --------------- 30.7/52.8 kB 640.0 kB/s eta 0:00:01\n",
      "     ---------------------- --------------- 30.7/52.8 kB 640.0 kB/s eta 0:00:01\n",
      "     ---------------------- --------------- 30.7/52.8 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 52.8/52.8 kB 302.1 kB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-win_amd64.whl.metadata (116 kB)\n",
      "     ---------------------------------------- 0.0/116.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/116.4 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 20.5/116.4 kB ? eta -:--:--\n",
      "     ------------------- ----------------- 61.4/116.4 kB 812.7 kB/s eta 0:00:01\n",
      "     ----------------------------- ------- 92.2/116.4 kB 751.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 116.4/116.4 kB 753.2 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\josep\\miniconda3\\lib\\site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\josep\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (26.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.1.1-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\josep\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\josep\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------------------------------- 0.1/8.1 MB 656.4 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.2/8.1 MB 1.5 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/8.1 MB 1.7 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/8.1 MB 1.7 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.7 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/8.1 MB 1.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.7/8.1 MB 1.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.7/8.1 MB 1.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.7/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.8/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.9/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.9/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.0/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.1/8.1 MB 1.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.2/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.2/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.3/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.4/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.5/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.0/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.2/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.2/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.3/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.3/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.5/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.5/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.6/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.7/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.7/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.8/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.0/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.1/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.1/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.2/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.3/8.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.8/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.9/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.0/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.2/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.4/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.4/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.6/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.7/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.8/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.9/8.1 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/8.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/8.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.3/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.4/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.5/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.6/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.6/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.7/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.9/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.9/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.1/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.3/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.3/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.4/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.5/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.7/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.9/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.0/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.0/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.3/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.5/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.7/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.8/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "   ---------------------------------------- 0.0/226.6 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 61.4/226.6 kB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 143.4/226.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/226.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 226.6/226.6 kB 1.5 MB/s eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.3 MB 2.0 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.3 MB 1.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.3 MB 1.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.3 MB 1.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.3 MB 1.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.3 MB 1.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.5/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.7/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "   ---------------------------------------- 0.0/73.9 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 30.7/73.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 73.9/73.9 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading pillow-12.1.1-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/7.0 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.1/7.0 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.2/7.0 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.0 MB 2.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.0 MB 2.0 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.7/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.9/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.1/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.3/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.4/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.5/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.6/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.7/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.7/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.9/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.0/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.1/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.2/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.3/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.3/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.4/7.0 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.4/7.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.4/7.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.5/7.0 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.5/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.5/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.7/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.7/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.8/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.0/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.1/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.2/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.3/7.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.4/7.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.5/7.0 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.5/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.6/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.7/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.9/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.0/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.1/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.3/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.4/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.5/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.6/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.7/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.8/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.9/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.1/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.3/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.3/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.4/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.6/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.7/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.9/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.0/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.1/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.2/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.2/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.3/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.4/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.5/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.7/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.9/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 1.8 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.1 pyparsing-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ad0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 3145728 bytes (66009944 bytes left)...\n",
      "Resuming download to C:\\Users\\Josep\\.cache\\kagglehub\\datasets\\mlg-ulb\\creditcardfraud\\3.archive (3145728/69155672) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66.0M/66.0M [00:29<00:00, 2.22MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Josep\\.cache\\kagglehub\\datasets\\mlg-ulb\\creditcardfraud\\versions\\3\n"
     ]
    }
   ],
   "source": [
    "# # Download latest version\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587b68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT NECESSARY LIBRARIES FOR CLASSIFICATION TASK, MODEL ANALYSIS, FEATURE SCALING AND DECISIONS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62bb152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (284807, 31)\n",
      "\n",
      "First few rows:\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Class Distribution (BEFORE balancing):\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Imbalance Ratio:\n",
      "Fraud: 492 (0.17%)\n",
      "Legitimate: 284315 (99.83%)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/Josep/.cache/kagglehub/datasets/mlg-ulb/creditcardfraud/versions/3/creditcard.csv')\n",
    "#load the data\n",
    "\n",
    "#explore the data \n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nClass Distribution (BEFORE balancing):\")\n",
    "print(df['Class'].value_counts())\n",
    "print(\"\\nClass Imbalance Ratio:\")\n",
    "print(f\"Fraud: {df['Class'].sum()} ({100 * df['Class'].sum() / len(df):.2f}%)\")\n",
    "print(f\"Legitimate: {(df['Class'] == 0).sum()} ({100 * (df['Class'] == 0).sum() / len(df):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d947b",
   "metadata": {},
   "source": [
    "After inspecting the data, it is clear that the dataset primarly consists of legitimate classes and not many fraudulent classes.\n",
    "\n",
    "\n",
    "This is an issue becuase as the number of frauds is so low, the model will train on more or less legitmate classes only, this is known as undersampling. \n",
    "\n",
    "\n",
    "To prevent this, a function called SMOTE creates new synthetic fraud examples using geometric points between a fraud transaction and its nearest neighbour. This way the model learns to be able to differentiate the boundary between fraud and legitimate more clearly. \n",
    "\n",
    "Additionally, the data consists of a time series aspect which can be removed as this is a cross sectional prediction only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06c0a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features shape: (284807, 29)\n",
      "distribution: {0: 284315, 1: 492}\n"
     ]
    }
   ],
   "source": [
    "#Drop Time  due to timne series natire of the data.\n",
    "#Instead, perform cross-sectional analysis\n",
    "X = df.drop(['Time', 'Class'], axis=1)\n",
    "# predict Class\n",
    "y = df['Class']\n",
    "#print features\n",
    "print(\"\\nFeatures shape:\", X.shape)\n",
    "print(\"distribution:\", y.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1688b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train, test split, preserving the ratio between train \n",
    "#and test having the same fraud to legitimate and using random state for \n",
    "#reproducability\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5445290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set size: 227845\n",
      "Test set size: 56962\n",
      "Training fraud ratio: 0.0017\n",
      "Test fraud ratio: 0.0017\n"
     ]
    }
   ],
   "source": [
    "#Output results to confirm accuracy of the previous code\n",
    "print(f\"\\nTrain set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Training fraud ratio: {y_train.sum() / len(y_train):.4f}\")\n",
    "print(f\"Test fraud ratio: {y_test.sum() / len(y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f0c55",
   "metadata": {},
   "source": [
    "**SMOTE**\n",
    "\n",
    "\n",
    "In highly unbalanced classification settings, models tend to be biased toward the majority of cases, leading to bad detection of minorities(Chawla et al., 2002; Fernández et al., 2018). It works by generating synthetic minority samples along the line that joins each minority instance to its nearest kst minority neighbour. This increases minority density without duplicating pre existing minorities, which is important to prevent overfitting issues(Chawla et al., 2002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da57462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution AFTER SMOTE:\n",
      "Class\n",
      "0    227451\n",
      "1    227451\n",
      "Name: count, dtype: int64\n",
      "Training set expanded from 227845 to 454902 samples\n",
      "New training fraud ratio: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#SMOTE implementation using 5 closest fradulent neighbours\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "print(\"\\nClass Distribution AFTER SMOTE:\")\n",
    "print(y_train_balanced.value_counts())\n",
    "#New training data to prevent overfittinf from SMOTE\n",
    "print(f\"Training set expanded from {len(y_train)} to {len(y_train_balanced)} samples\")\n",
    "print(f\"New training fraud ratio: {y_train_balanced.sum() / len(y_train_balanced):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53e5c6",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "MLP models are sensitive to scaling. This is because when all input features are on a similar scale, the model can train more reliably and faster due to large magnitutes features not dominating the learning dynamics.Many Scikit learn estimators assume scaled input features. Standard Scalar stanadardizes each feature to zero mean and unit variance (scikit‑learn developers, 2006; GeeksforGeeks, 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce2b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature mean: [-6.39782425e-17  0.00000000e+00  0.00000000e+00]\n",
      "Training feature std: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "#Fit and scale the training data.\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Confirm using mean and standard deviation of the training data\n",
    "print(f\"Training feature mean: {X_train_scaled.mean(axis=0)[:3]}\")\n",
    "print(f\"Training feature std: {X_train_scaled.std(axis=0)[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40910016",
   "metadata": {},
   "source": [
    "**Model Selection and training**\n",
    "\n",
    "MLP works well for credit card fraud prediction because it can find and predict using complex non linear patterns that a simple model like logistic regression wont be able to capture. When compared to other classical models, MLP has been able to outperfrom on precison, recall, accuracy, and f1-score (Edelweiss Applied Science and Technology, 2025). \n",
    "\n",
    "This specific set up uses a two layer hidden architecture, an adpative learning rate, and the Relu activation function. It also uses regularisation to help prevent overfitting, through penalising model complexity by using an extra term on the loss function which forces the model towards smaller and simpler parameter values. \n",
    "\n",
    "At the present time it isnt known whether these parameters are optimal, this can be determined using an algorithm like Gridsearch or the genetic algorithm, while being careful to prevent over fitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a580df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Training complete.\n",
      "Converged: 19 iterations\n",
      "Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam', #adam \n",
    "    alpha=1e-4, #regularisation term\n",
    "    batch_size=256, #small batch to help better generalisation.\n",
    "    learning_rate='adaptive', #adaptive learning rate to increase learning efficiency\n",
    "    learning_rate_init=1e-3, \n",
    "    early_stopping=True, #Stop early if no improvment.\n",
    "    validation_fraction=0.1,  # Use 10% of training for early stopping\n",
    "    n_iter_no_change=10,  # Stop if no improvement for 10 checks\n",
    "    max_iter=200,     #Maximium epoch number\n",
    "    random_state=42, # reproducability\n",
    "    verbose=False   #No training information printed to the screen.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp.fit(X_train_scaled, y_train_balanced) # Fit the model\n",
    "#training output metrics including model fit\n",
    "print(f\"Training complete.\")\n",
    "print(f\"Converged: {mlp.n_iter_} iterations\")\n",
    "print(f\"Loss: {mlp.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33938b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform prediction on the test data.\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "#probability for ROC-AUC calculation\n",
    "y_pred_proba = mlp.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16246203",
   "metadata": {},
   "source": [
    "**Training results**\n",
    "Using a confusion matrix and the true and false predictions, industry standard assesemnt metrics can evalaute how well the model performs on unseen data. \n",
    "\n",
    "A confusion matrix is a table that shows how many fradulent vs not fradulent predictions that the model correctly and falsely identified. \n",
    "\n",
    "Defining the evaluation metrics:\n",
    "\n",
    "*Accuracy* is true positives + true negatives/all samples\n",
    "\n",
    "*Precision* is true positives / true positives + false positives\n",
    "\n",
    "*Recall* is true positives / true positives + false negatives\n",
    "\n",
    "*F1-Score*  is known as the harmonic mean of precision and recall it is equal to 2 * precision * recall / precision + recall. It summarises the trade-off between precision and recall into a single metric.\n",
    "\n",
    "*ROC-AUC score* shows how the classifiers true positive rate compares to the false positive rates across all possible probability thresholds.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec7d6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00     56864\n",
      "       Fraud       0.66      0.83      0.74        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.83      0.91      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives: 56823\n",
      "False Positives: 41\n",
      "False Negatives: 17\n",
      "True Positives: 81\n",
      "\n",
      "Key Metrics:\n",
      "Accuracy: 0.9990\n",
      "Precision: 0.6639  (avoid false alarms)\n",
      "Recall: 0.8265    (catch frauds)\n",
      "F1-Score: 0.7364\n",
      "ROC-AUC: 0.9607\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "print(f\"Accuracy: {(tp + tn) / (tp + tn + fp + fn):.4f}\")\n",
    "print(f\"Precision: {tp / (tp + fp):.4f}  (avoid false alarms)\")\n",
    "print(f\"Recall: {tp / (tp + fn):.4f}    (catch frauds)\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505797ef",
   "metadata": {},
   "source": [
    "**Results interpretation**\n",
    "\n",
    "From the confusion matrix, it is clear that when the model flags fraud, it is right 81% of the time. It catches 84% of all actual frauds, missing 16 out of 98.\n",
    "\n",
    "The f1 score displays 0.82 which indicates a good balance between catching fraud and not spamming false alerts. \n",
    "\n",
    "A ROC-AUC score of 0.96 shows a good ranking ability in that the model is good at seperating fraud vs legitimate across thresholds.\n",
    "\n",
    "These metrics dont suggest overfitting, since they demonstrate the model performs well on the unseen test data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2a5cb",
   "metadata": {},
   "source": [
    "**Furthering the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eca597",
   "metadata": {},
   "source": [
    "Adding a dashboard using something like streamlit api, just as Sarunas used, alongside data collection would create a real way for users to interact with the model, taking it from a basic concept to production. The data collection addition would mean that the model could be further refined in future iterations.\n",
    "\n",
    "To further the model performance, an optimal set of hyperparameters need to be chosen through a search-optimisation algorithm, like the forementioned genetic algorithm that incoporates a fitness function which determines the best \"traits\" of the parents and passes them on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee026c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Best params: {'alpha': 1e-05, 'batch_size': 512, 'hidden_layer_sizes': (256, 128, 64), 'learning_rate_init': 0.0005}\n",
      "Best CV F1: 0.9997604480271484\n",
      "\n",
      "Optimal model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00     56864\n",
      "       Fraud       0.72      0.83      0.77        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.86      0.91      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define parameter grid based on current MLP setup\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(128, 64), (256, 128), (256, 128, 64)],\n",
    "    'alpha': [1e-5, 1e-4, 1e-3],\n",
    "    'learning_rate_init': [1e-4, 5e-4, 1e-3],\n",
    "    'batch_size': [128, 256, 512]\n",
    "}\n",
    "\n",
    "# MLP base estimator (fix common params)\n",
    "mlp_base = MLPClassifier(\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Scorer for imbalanced data (F1 for fraud class)\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# GridSearchCV with 3-fold CV (StratifiedKFold already imported)\n",
    "grid_search = GridSearchCV(\n",
    "    mlp_base, param_grid, cv=3, scoring=f1_scorer, n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Fit on balanced scaled train data\n",
    "print(\"Running GridSearchCV...\")\n",
    "grid_search.fit(X_train_scaled, y_train_balanced)\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV F1:\", grid_search.best_score_)\n",
    "\n",
    "# Best model for test evaluation\n",
    "best_mlp = grid_search.best_estimator_\n",
    "ypred_opt = best_mlp.predict(X_test_scaled)\n",
    "print(\"\\nOptimal model classification report:\")\n",
    "print(classification_report(y_test, ypred_opt, target_names=['Legitimate', 'Fraud']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a5fda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fraud_scaler.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(grid_search.best_estimator_, 'fraud_mlp_model.joblib')\n",
    "joblib.dump(scaler, 'fraud_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38db731",
   "metadata": {},
   "source": [
    "**3.3 Bibliography**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80265564",
   "metadata": {},
   "source": [
    "Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16, 321–357.​\n",
    "\n",
    "Fernández, A., García, S., Herrera, F., & Chawla, N. V. (2018). SMOTE for learning from imbalanced data: Progress and challenges, marking the 15-year anniversary. Journal of Artificial Intelligence Research, 61, 863–905.\n",
    "\n",
    "GeeksforGeeks. (2018, July 1). Feature engineering: Scaling, normalization and standardization.​\n",
    "\n",
    "scikit-learn developers. (2006). StandardScaler — scikit-learn 1.8.0 documentation.\n",
    "\n",
    "Edelweiss Applied Science and Technology. (2025). Fraud credit card transaction detection using hybrid multilayer perceptron models. Edelweiss Applied Science and Technology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
